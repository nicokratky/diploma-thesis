\chapter{Data Analysis}
\label{ch:dataanalysis}

\section {Regression Analysis}

Regression analysis is a statistical method to determine relationships between a response variable $ y $ and one or more predictor variables $ x_i $, where $ i = 1, 2, ..., p $. Linear regression analysis assumes that these predictor variables are related linear to the response variable.

\subsection{Simple Linear Regression}

Simple Linear Regression is a regression model that can only build a relationship between one predictor variable and one response variable. To find the best fit for this linear model the \textit{Ordinary Least Squares} method is used. The linear regression model builds a linear function

\begin{equation}
\label{eq:line}
    y = k * x + d
\end{equation}

that represents the predicted values. This function can also be denoted as

\begin{equation}
\label{eq:slr}
    y = \beta_0 + \beta_1 * x,
\end{equation}

where $ \beta_0 $ is the intercept and $ \beta_1 $ is the slope of the line.

The two regression parameters that are used in this regression model are calculated using the least squares method. This method tries to minimize the sum of squared residuals. Equations \vref{eq:beta1} and \vref{eq:beta0} show these procedures.

\begin{equation}
\label{eq:beta1}
    \beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x}) * (y_i - \bar{y}))}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
\end{equation}

\begin{equation}
    \label{eq:beta0}
    \beta_0 = \bar{y} - \beta_1 * \bar{x}
\end{equation}

\subsection{Multiple Linear Regression}

When the dependent variable depends on not just one variable, multiple linear regression analysis is used. This method uses two or more independent variables to describe the dependent variable. To calculate the regression coefficients the predictor variables have to be put into a $ n \times p $ matrix,

\begin{equation}
    X_{n,p} =
        \begin{bmatrix}
            1 & x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
            1 & x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
            1 & \vdots & \vdots & \ddots & \vdots \\
            1 & x_{n,1} & x_{n,2} & \cdots & x_{n,p}
        \end{bmatrix}
\end{equation}

where $ n $ is the amount of datasets and $ p $ is the amount of predictor variables $ + 1 $ because the intercept also has to be calculated. Also, a vector of all response variables

\begin{equation}
    y_{n} =
        \begin{bmatrix}
            y_{1} \\
            y_{2} \\
            \vdots \\
            y_{n}
        \end{bmatrix}
\end{equation}

has to be formed. These two matrices can be used to describe the basic multiple linear regression model

\begin{equation}
    \begin{bmatrix}
        y_{1} \\
        y_{2} \\
        \vdots \\
        y_{p}
    \end{bmatrix}
    =
    \begin{bmatrix}
            1 & x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
            1 & x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
            1 & \vdots & \vdots & \ddots & \vdots \\
            1 & x_{n,1} & x_{n,2} & \cdots & x_{n,p}
    \end{bmatrix}
    \begin{bmatrix}
        \beta_{0} \\
        \beta_{1} \\
        \vdots \\
        \beta_{p}
    \end{bmatrix}
\end{equation}

where $ \beta $ are the regression coeffecients. Or shorter

\begin{equation}
    y = X\beta,
\end{equation}

\todo{describe what leads to the formula below}

\begin{equation}
\label{eq:mlr}
    \hat{\beta} = (X^TX)^{-1} X^Ty
\end{equation}

\subsection{Example - Ringsize of Women}

A good example for multiple linear regression analysis is the ringsize of women. If somebody wants to know the ringsize of his or her girlfriend, but does not want to ask her it is possible to predict her size. To be able to do this a data basis has to be formed. Decisive factors for someones ringsize are for example: body weight, height and age. These variables can be used to form said data basis as shown in figure \vref{tab:mlr_ringsize}.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
    \textbf{Person $ i $}    & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} \\ \hline
    \textbf{Ringsize $ y $}  & 47.1       & 46.8       & 49.3       & 53.2       & 47.7       & 49.0       & 50.6       & 47.1       & 51.7       & 47.8        \\ \hline
    \textbf{Height $ x_1 $}  & 156.3      & 158.9      & 160.8      & 179.6      & 156.6      & 165.1      & 165.9      & 156.7      & 167.8      & 160.8       \\ \hline
    \textbf{Weight $ x_2 $}  & 62         & 52         & 83         & 69         & 74         & 52         & 77         & 65         & 79         & 51          \\ \hline
    \textbf{Age $ x_3 $}     & 24         & 34         & 26         & 51         & 43         & 33         & 22         & 21         & 19         & 34          \\ \hline
    \end{tabular}
    \caption{Ringsizes of example persons and the appropriate body parameters}
    \label{tab:mlr_ringsize}
\end{table}

This dataset can now be used to form the previously explained matrices.

\begin{equation}
    X_{10, 3} =
    \begin{bmatrix}
        1 & 156.3 & 62 & 24 \\
        1 & 158.9 & 52 & 34 \\
        \vdots & \vdots & \vdots & \vdots \\
        1 & 160.8 & 51 & 34
    \end{bmatrix}
\end{equation}

\begin{equation}
    y_{10} =
    \begin{bmatrix}
        47.1 \\
        46.8 \\
        \vdots \\
        47.8
    \end{bmatrix}
\end{equation}

Using the regression parameter formula (see equation \vref{eq:mlr}) we get the following regression parameters:

\begin{equation}
    \hat{\beta} = (X^TX)^{-1} X^Ty =
    \begin{bmatrix}
        0.66 \\
        0.28 \\
        0.06 \\
        -0.02
    \end{bmatrix}
\end{equation}

Using these parameters it is now possible to form the regression function which allows for data prediction

\begin{equation}
    y = 0.66 + 0.28 * x_1 + 0.06 * x_2 - 0.02 * x_3
\end{equation}

Lets assume that a woman is 170cm tall, weighs 68kg and is 29 years old. Inserting these values into the calculated model

\begin{equation}
    y = 0.66 + 0.28 * 170 + 0.06 * 68 - 0.02 * 29
\end{equation}

gives us the predicted ringsize $ y = 51.76 $.

\subsection{Working with Streaming Data}

% schwieriger weil nicht immer alle daten zur verfuegung stehen
% coefficienten muessen laufen geaendert werden

% M und V

As GRAMOC uses streaming data a few adjustements had to be made. The ordinar multiple linear regression model assumes that all observations are available when calculating the regression coefficients. Therefore a way of calculating these regression coefficients in a streaming way had to be found. To accomplish this the regression parameters have to be calculated incrementally. This means that the two parts of the regression parameter calculation, $ X^T X $ and $ X^T y $ have to be recalculated every time a new observation is made. These results then have to be added to the existing matrices. This is possible as $ X^T X $ always returns a $ p \times p $ matrix and $ X^T y $ always returns a $ p $-dimensional vector.

\todo{cite StreamFitter}

\begin{equation}
    \hat{\beta_k} = (M + {X_k}^T X_k)^{-1}(V + {X_k}^T y_k)
\end{equation}

\subsection{$ R^2 $ - Coefficient of Determination}

\begin{equation}
    R^2 = \frac{\sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{equation}

\section{Implementation}
