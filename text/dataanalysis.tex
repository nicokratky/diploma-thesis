\chapter{Data Analysis}
\label{ch:dataanalysis}

\author{Nico Kratky}
%
\section {Regression Analysis}

Regression analysis is a statistical method to determine relationships between a response variable $ y $ and one or more predictor variables $ x_i $, where $ i = 1, 2, ..., p $. Linear regression analysis assumes that these predictor variables are related linear to the response variable.

\subsection{Simple Linear Regression}
\label{sec:slr}

Simple Linear Regression is a regression model that can only build a relationship between one predictor variable and one response variable. To find the best fit for this linear model the \textit{Ordinary Least Squares} method is used. The linear regression model builds a linear function

\begin{equation}
\label{eq:line}
    y = k * x + d
\end{equation}

that represents the predicted values. This function can also be denoted as

\begin{equation}
\label{eq:slr}
    y = \beta_0 + \beta_1 * x,
\end{equation}

where $ \beta_0 $ is the intercept and $ \beta_1 $ is the slope of the line.

The two regression parameters,

\begin{equation}
    \beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x}) * (y_i - \bar{y}))}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
\end{equation}

\begin{equation}
    \beta_0 = \bar{y} - \beta_1 * \bar{x},
\end{equation}

that are used in this regression model are calculated using the least squares method. This method tries to minimize the sum of squared residuals.

\subsubsection{Example}

A good example for linear regression analysis is the ringsize of women. This example was taken from the website \url{http://www.crashkurs-statistik.de} \autocite{CrashkursSLR} \autocite{CrashkursMLR}.

If somebody wants to know the ringsize of his girlfriend, but does not want to ask her, it is possible to predict the size. To be able to do this a data basis has to be formed. A decisive factor for someones ringsize is for example the body height.

The data, which is depicted in table \vref{tab:slr_ringsize} can be used to calculate the regression coefficients using the formulae discussed in section \vref{sec:slr}. These calculations result in the two regression coefficients

\begin{equation}
    \beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x}) * (y_i - \bar{y}))}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = 0.2838
\end{equation}

\begin{equation}
    \beta_0 = \bar{y} - \beta_1 * \bar{x} = 2.8457,
\end{equation}

which are slope and intercept, respectively.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
    \textbf{Person $ i $}    & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} \\ \hline
    \textbf{Ringsize $ y $}  & 47.1       & 46.8       & 49.3       & 53.2       & 47.7       & 49.0       & 50.6       & 47.1       & 51.7       & 47.8        \\ \hline
    \textbf{Height $ x $}    & 156.3      & 158.9      & 160.8      & 179.6      & 156.6      & 165.1      & 165.9      & 156.7      & 167.8      & 160.8       \\ \hline
    \end{tabular}
    \caption{Ringsizes of example persons and their body heights}
    \label{tab:slr_ringsize}
\end{table}

After these calculations, the regression line can be plotted as shown in figure \vref{fig:slr_ringsize}.

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    axis lines = left,
    xlabel = {Height ($x$)},
    ylabel = {Ringsize ($y$)},
]
\addplot [
    domain=155:180,
    color=red
]
{2.8457 + 0.2836 * x};

\addplot[
    only marks
]
table {
    x       y
    156.3   47.1
    158.9   46.8
    160.8   49.3
    179.6   53.2
    156.6   47.7
    165.1   49.0
    165.9   50.6
    156.7   47.1
    167.8   51.7
    160.8   47.8
};

\end{axis}
\end{tikzpicture}

\caption{Ringsizes of women. The single data points represents the gathered data. The red line depicts the regression line.}
\label{fig:slr_ringsize}
\end{figure}

To predict a persons ringsize, two options are available. The first option, which is not as accurate as the second one, is to simple read the predicted value from the regression line. If, for example, a person is 165cm tall, the proper ringsize would be a little bit less than 50. To be more precise, the second option can be used, which is to calculate the size using the regression coefficients. This can be done by applying the regression line to the height

\begin{equation}
    y = 2.8457 + 0.2836 * 165,
\end{equation}

which yields $ y = 49.64 $.

\subsection{Multiple Linear Regression}
\label{sec:mlr}

When the dependent variable depends on not just one variable, multiple linear regression analysis is used. This method uses two or more independent variables to describe the dependent variable. To calculate the regression coefficients the predictor variables have to be put into a $ n \times p $ matrix,

\begin{equation}
    X_{n,p} =
        \begin{bmatrix}
            1 & x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
            1 & x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
            1 & \vdots & \vdots & \ddots & \vdots \\
            1 & x_{n,1} & x_{n,2} & \cdots & x_{n,p}
        \end{bmatrix}
\end{equation}

where $ n $ is the amount of datasets and $ p $ is the amount of predictor variables $ + 1 $ because the intercept also has to be calculated. Also, a vector of all response variables

\begin{equation}
    y_{n} =
        \begin{bmatrix}
            y_{1} \\
            y_{2} \\
            \vdots \\
            y_{n}
        \end{bmatrix}
\end{equation}

has to be formed. These two matrices can be used to describe the basic multiple linear regression model

\begin{equation}
    \begin{bmatrix}
        y_{1} \\
        y_{2} \\
        \vdots \\
        y_{p}
    \end{bmatrix}
    =
    \begin{bmatrix}
            1 & x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
            1 & x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
            1 & \vdots & \vdots & \ddots & \vdots \\
            1 & x_{n,1} & x_{n,2} & \cdots & x_{n,p}
    \end{bmatrix}
    \begin{bmatrix}
        \beta_{0} \\
        \beta_{1} \\
        \vdots \\
        \beta_{p}
    \end{bmatrix},
\end{equation}

where $ \beta $ are the regression coeffecients. Or shorter

\begin{equation}
    y = X\beta.
\end{equation}

\todo{describe what leads to the formula below}

The problem with this equation is that it is possible that this equation does not have a solution. Therefore the $ y $ and $ X $ matrices are multiplied with the transpose of $ X $.

\begin{equation}
\label{eq:mlr}
    \hat{\beta} = (X^TX)^{-1} X^Ty
\end{equation}

\subsubsection{Example}

With this in mind, the example from section \vref{sec:slr} can be extended by more body parameters like weight and age, as they may also have an impact on the accuracy of the regression model. Figure \vref{tab:mlr_ringsize} depicts the ringsizes and body heights from the previous example, plus the weight and age of these people.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
    \textbf{Person $ i $}    & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} \\ \hline
    \textbf{Ringsize $ y $}  & 47.1       & 46.8       & 49.3       & 53.2       & 47.7       & 49.0       & 50.6       & 47.1       & 51.7       & 47.8        \\ \hline
    \textbf{Height $ x_1 $}  & 156.3      & 158.9      & 160.8      & 179.6      & 156.6      & 165.1      & 165.9      & 156.7      & 167.8      & 160.8       \\ \hline
    \textbf{Weight $ x_2 $}  & 62         & 52         & 83         & 69         & 74         & 52         & 77         & 65         & 79         & 51          \\ \hline
    \textbf{Age $ x_3 $}     & 24         & 34         & 26         & 51         & 43         & 33         & 22         & 21         & 19         & 34          \\ \hline
    \end{tabular}
    \caption{Ringsizes of example persons and the appropriate body parameters}
    \label{tab:mlr_ringsize}
\end{table}

This dataset can now be used to form the previously explained matrices.

\begin{equation}
    X_{10, 3} =
    \begin{bmatrix}
        1 & 156.3 & 62 & 24 \\
        1 & 158.9 & 52 & 34 \\
        \vdots & \vdots & \vdots & \vdots \\
        1 & 160.8 & 51 & 34
    \end{bmatrix}
\end{equation}

\begin{equation}
    y_{10} =
    \begin{bmatrix}
        47.1 \\
        46.8 \\
        \vdots \\
        47.8
    \end{bmatrix}
\end{equation}

Using the regression parameter formula (see equation \vref{eq:mlr}) we get the regression parameters

\begin{equation}
    \hat{\beta} = (X^TX)^{-1} X^Ty =
    \begin{bmatrix}
        0.66 \\
        0.28 \\
        0.06 \\
        -0.02
    \end{bmatrix}.
\end{equation}

Using these parameters it is now possible to form the regression function

\begin{equation}
    y = 0.66 + 0.28 * x_1 + 0.06 * x_2 - 0.02 * x_3,
\end{equation}

which allows for data prediction.

In multiple linear regression analysis the predicted value can no longer be read off a graph, as the line is multi-dimensional. Lets assume that a woman is 170cm tall, weighs 68kg and is 29 years old. Inserting these values into the calculated model

\begin{equation}
    y = 0.66 + 0.28 * 170 + 0.06 * 68 - 0.02 * 29
\end{equation}

gives us the predicted ringsize $ y = 51.76 $.

\subsection{Working with Streaming Data}

As GRAMOC uses streaming data a few adjustements had to be made. The ordinary multiple linear regression model assumes that all observations are available when calculating the regression coefficients. Therefore a way of calculating these regression coefficients in a streaming way had to be found. To accomplish this the regression parameters have to be calculated incrementally. This means that the two parts of the regression parameter calculation, $ X^T X $ and $ X^T y $ have to be recalculated every time a new observation is made. These two matrices are then added up, since matrix addition between two $ n \times n$ matrices also result in a $ n \times n$ matrix. The sum of these matrices are named $ M $ and $ V $. This is possible as $ X^T X $ always returns a $ p \times p $ matrix and $ X^T y $ always returns a $ p $-dimensional vector.

When the regression coefficients are calculated,

\begin{equation}
    \hat{\beta_k} = (M + {X_k}^T X_k)^{-1}(V + {X_k}^T y_k),
\end{equation}

the sum of previous observation data is added to the current data, and then the procedure described in \vref{sec:mlr} can be applied.

This procedure was introduced in a regression calculation programm called StreamFitter, which also works with streaming data \autocite{StreamFitter}.

\subsection{$ r^2 $ - Coefficient of Determination}

$ r^2 $ is the quotient of the explained variation

\begin{equation}
    ESS = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2
\end{equation}

and the total variation

\begin{equation}
    TSS = \sum_{i=1}^{n} (y_i - \bar{y})^2,
\end{equation}

\begin{equation}
    r^2 = \frac{ESS}{TSS}
\end{equation}

It is a measure for how much variation of the data can be explained with this regression model. $ r^2 $ values are ranged between $ 0 $ and $ 1 $, where $ 1 $ is considered a perfect fit and $ 0 $ says that no data can be explained using this regressino model.

\section{Implementation}

